{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "# let's not pollute this blog post with warnings\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runModel(X, Y):\n",
    "    x, y = X[:24000], Y[:24000]\n",
    "    x_val, y_val = X[24000:27000], Y[24000:27000]\n",
    "    x_test, y_test = X[27000:], Y[27000:]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, activation='sigmoid', input_dim=x.shape[1]))\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "    model.fit(x, y, nb_epoch=10, validation_data=(x_val, y_val))\n",
    "    y_test = np.reshape(y_test,(-1))\n",
    "    y_pred = np.reshape(model.predict(x_test),(-1))\n",
    "    return (np.sum(y_test != y_pred)*1.0)/3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.7269 - val_loss: 0.5018\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4994 - val_loss: 0.4601\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4840 - val_loss: 0.4536\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4801 - val_loss: 0.4514\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4788 - val_loss: 0.4505\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4782 - val_loss: 0.4503\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4780 - val_loss: 0.4502\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4777 - val_loss: 0.4487\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4777 - val_loss: 0.4491\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4775 - val_loss: 0.4490\n",
      "Train on 24000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 2s - loss: 1.6694 - val_loss: 0.5528\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.5406 - val_loss: 0.4870\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4986 - val_loss: 0.4654\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4863 - val_loss: 0.4593\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4822 - val_loss: 0.4548\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4804 - val_loss: 0.4543\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4794 - val_loss: 0.4547\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4790 - val_loss: 0.4543\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4785 - val_loss: 0.4521\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4784 - val_loss: 0.4514\n",
      "Train on 24000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 2s - loss: 5.0434 - val_loss: 4.5851\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 1s - loss: 4.5340 - val_loss: 3.8101\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 1s - loss: 3.9009 - val_loss: 3.4496\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 1s - loss: 3.6341 - val_loss: 3.1956\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 1s - loss: 3.5407 - val_loss: 3.2844\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 1s - loss: 3.5235 - val_loss: 3.1934\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 1s - loss: 3.5334 - val_loss: 3.1790\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 1s - loss: 3.5301 - val_loss: 3.2614\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 1s - loss: 3.5354 - val_loss: 3.2165\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 1s - loss: 3.5303 - val_loss: 3.1488\n",
      "Train on 24000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 2s - loss: 12.3700 - val_loss: 12.7380\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 1s - loss: 12.3700 - val_loss: 12.7380\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 1s - loss: 12.3700 - val_loss: 12.7380\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 1s - loss: 12.3700 - val_loss: 12.7380\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 1s - loss: 12.3700 - val_loss: 12.7380\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 1s - loss: 12.3700 - val_loss: 12.7380\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 1s - loss: 12.3700 - val_loss: 12.7380\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 1s - loss: 12.3700 - val_loss: 12.7380\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 2s - loss: 12.3700 - val_loss: 12.7380\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 2s - loss: 12.3700 - val_loss: 12.7380\n",
      "Train on 24000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.6972 - val_loss: 0.5193\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.5204 - val_loss: 0.4585\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4835 - val_loss: 0.4415\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4723 - val_loss: 0.4346\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4683 - val_loss: 0.4326\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4661 - val_loss: 0.4313\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4649 - val_loss: 0.4308\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4641 - val_loss: 0.4306\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4641 - val_loss: 0.4304\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4638 - val_loss: 0.4293\n",
      "Train on 24000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.5714 - val_loss: 0.4591\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4922 - val_loss: 0.4437\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4792 - val_loss: 0.4406\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4736 - val_loss: 0.4366\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4709 - val_loss: 0.4385\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4691 - val_loss: 0.4351\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4674 - val_loss: 0.4377\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4670 - val_loss: 0.4335\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4660 - val_loss: 0.4326\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4658 - val_loss: 0.4405\n",
      "Train on 24000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.5239 - val_loss: 0.4617\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4895 - val_loss: 0.4548\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4838 - val_loss: 0.4531\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4809 - val_loss: 0.4507\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4792 - val_loss: 0.4491\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4783 - val_loss: 0.4492\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4778 - val_loss: 0.4498\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4775 - val_loss: 0.4490\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4772 - val_loss: 0.4487\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4770 - val_loss: 0.4490\n"
     ]
    }
   ],
   "source": [
    "smallestLabelPath = os.path.join('../data/smallest.csv')\n",
    "smallerLabelPath = os.path.join('../data/smaller.csv')\n",
    "smallLabelPath = os.path.join('../data/small.csv')\n",
    "positiveSmallLabelPath = os.path.join('../data/positive_small.csv')\n",
    "allDiscretizedPath = os.path.join('../data/all_features_discretized.csv')\n",
    "positiveAllDiscretizedPath = os.path.join('../data/positive_all_discretized.csv')\n",
    "discreteOnlyPath = os.path.join('../data/discrete_features.csv')\n",
    "\n",
    "dataTypes = [\"smallest\", \"smaller\", \"small\", \"pos small\", \"all disc\", \"pos all disc\", \"disc only\"]\n",
    "paths = [smallestLabelPath, smallerLabelPath, smallLabelPath, positiveSmallLabelPath, allDiscretizedPath, positiveAllDiscretizedPath, discreteOnlyPath]\n",
    "\n",
    "accuracies = []\n",
    "for i, path in enumerate(paths):\n",
    "    data = pd.read_csv(path)\n",
    "    array = np.array(data)\n",
    "    \n",
    "    X = array[:,:array.shape[1]-1]\n",
    "    Y = array[:,array.shape[1]-1]\n",
    "    \n",
    "    accuracies.append(runModel(X,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 0.31233333333333335, 0.77866666666666662, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
