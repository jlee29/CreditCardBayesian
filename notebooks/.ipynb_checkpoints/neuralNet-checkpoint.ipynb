{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "# let's not pollute this blog post with warnings\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runModel(X, Y):\n",
    "    x, y = X[:24000], Y[:24000]\n",
    "    x_val, y_val = X[24000:27000], Y[24000:27000]\n",
    "    x_test, y_test = X[27000:], Y[27000:]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, activation='sigmoid', input_dim=x.shape[1]))\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "    model.fit(x, y, nb_epoch=10, validation_data=(x_val, y_val))\n",
    "    y_test = np.reshape(y_test,(-1))\n",
    "    y_pred = np.reshape(model.predict(x_test),(-1))\n",
    "    return (np.sum(y_test != y_pred)*1.0)/3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 2s - loss: 3.6085 - val_loss: 3.2397\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 2s - loss: 3.6086 - val_loss: 3.2397\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 2s - loss: 3.6082 - val_loss: 3.2397\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 2s - loss: 3.6074 - val_loss: 3.2397\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 2s - loss: 3.6066 - val_loss: 3.2397\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 2s - loss: 3.6060 - val_loss: 3.2397\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 2s - loss: 3.6057 - val_loss: 3.2397\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 2s - loss: 3.6054 - val_loss: 3.2397\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 2s - loss: 3.6051 - val_loss: 3.2397\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 2s - loss: 3.6050 - val_loss: 3.2397\n",
      "Train on 24000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 2s - loss: 3.6063 - val_loss: 3.2397\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 2s - loss: 3.6063 - val_loss: 3.2397\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 2s - loss: 3.6063 - val_loss: 3.2397\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 2s - loss: 3.6062 - val_loss: 3.2397\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 2s - loss: 3.6062 - val_loss: 3.2397\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 2s - loss: 3.6062 - val_loss: 3.2397\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 2s - loss: 3.6062 - val_loss: 3.2397\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 2s - loss: 3.6062 - val_loss: 3.2397\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 2s - loss: 3.6061 - val_loss: 3.2397\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 2s - loss: 3.6061 - val_loss: 3.2397\n",
      "Train on 24000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.7922 - val_loss: 0.4525\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4776 - val_loss: 0.4358\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4670 - val_loss: 0.4327\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4646 - val_loss: 0.4313\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4638 - val_loss: 0.4321\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4636 - val_loss: 0.4302\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4635 - val_loss: 0.4297\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4632 - val_loss: 0.4318\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4630 - val_loss: 0.4299\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4632 - val_loss: 0.4311\n",
      "Train on 24000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 2s - loss: 12.3530 - val_loss: 12.7380\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 2s - loss: 11.5345 - val_loss: 3.2381\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 2s - loss: 3.3420 - val_loss: 3.2332\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 2s - loss: 2.8158 - val_loss: 3.2333\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 2s - loss: 2.6064 - val_loss: 3.2345\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 2s - loss: 2.5218 - val_loss: 3.2397\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 2s - loss: 2.5031 - val_loss: 0.5773\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 2s - loss: 2.4481 - val_loss: 10.8388\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 2s - loss: 2.4212 - val_loss: 3.2397\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 2s - loss: 2.3843 - val_loss: 12.6466\n",
      "Train on 24000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.6326 - val_loss: 0.4981\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4980 - val_loss: 0.4652\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4842 - val_loss: 0.4570\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4805 - val_loss: 0.4540\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4789 - val_loss: 0.4517\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4781 - val_loss: 0.4516\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4778 - val_loss: 0.4513\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4775 - val_loss: 0.4506\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4771 - val_loss: 0.4501\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4773 - val_loss: 0.4496\n"
     ]
    }
   ],
   "source": [
    "smallLabelPath = os.path.join('../data/small.csv')\n",
    "positiveSmallLabelPath = os.path.join('../data/positive_small.csv')\n",
    "allDiscretizedPath = os.path.join('../data/all_features_discretized.csv')\n",
    "positiveAllDiscretizedPath = os.path.join('../data/positive_all_discretized.csv')\n",
    "discreteOnlyPath = os.path.join('../data/discrete_features.csv')\n",
    "\n",
    "dataTypes = [\"small\", \"pos small\", \"all disc\", \"pos all disc\", \"disc only\"]\n",
    "paths = [smallLabelPath, positiveSmallLabelPath, allDiscretizedPath, positiveAllDiscretizedPath, discreteOnlyPath]\n",
    "accuracies = []\n",
    "for i, path in enumerate(paths):\n",
    "    data = pd.read_csv(path)\n",
    "    array = np.array(data)\n",
    "    \n",
    "    X = array[:,:array.shape[1]-1]\n",
    "    Y = array[:,array.shape[1]-1]\n",
    "    \n",
    "    accuracies.append(runModel(X,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.221, 0.221, 1.0, 0.77933333333333332, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
