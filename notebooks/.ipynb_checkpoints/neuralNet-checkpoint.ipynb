{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "# let's not pollute this blog post with warnings\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runModel(X, Y):\n",
    "    x, y = X[:24000], Y[:24000]\n",
    "    x_val, y_val = X[24000:27000], Y[24000:27000]\n",
    "    x_test, y_test = X[27000:], Y[27000:]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, activation='sigmoid', input_dim=x.shape[1]))\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "    model.fit(x, y, nb_epoch=10, validation_data=(x_val, y_val))\n",
    "    y_test = np.reshape(y_test,(-1))\n",
    "    y_pred = np.reshape(model.predict(x_test),(-1))\n",
    "    return (np.sum(y_test != y_pred)*1.0)/3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 11s - loss: 0.6385 - val_loss: 0.4674\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4935 - val_loss: 0.4572\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4864 - val_loss: 0.4531\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4826 - val_loss: 0.4506\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4801 - val_loss: 0.4499\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4790 - val_loss: 0.4492\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4781 - val_loss: 0.4486\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4776 - val_loss: 0.4481\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4773 - val_loss: 0.4478\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4771 - val_loss: 0.4478\n",
      "Train on 24000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.5076 - val_loss: 0.4595\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4815 - val_loss: 0.4532\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4791 - val_loss: 0.4507\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4782 - val_loss: 0.4511\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4780 - val_loss: 0.4507\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4780 - val_loss: 0.4510\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4777 - val_loss: 0.4531\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4773 - val_loss: 0.4502\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4778 - val_loss: 0.4491\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4776 - val_loss: 0.4495\n",
      "Train on 24000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 1s - loss: 11.2322 - val_loss: 11.2301\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 1s - loss: 10.7678 - val_loss: 10.5402\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 1s - loss: 9.7060 - val_loss: 8.7650\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 1s - loss: 4.4348 - val_loss: 3.5889\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 1s - loss: 3.8472 - val_loss: 3.4613\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 1s - loss: 3.6827 - val_loss: 3.2580\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 1s - loss: 3.4391 - val_loss: 3.0075\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 1s - loss: 3.4950 - val_loss: 3.2069\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 1s - loss: 3.3341 - val_loss: 3.0140\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 1s - loss: 3.3482 - val_loss: 3.0007\n",
      "Train on 24000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.7022 - val_loss: 0.4660\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4848 - val_loss: 0.4402\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4716 - val_loss: 0.4386\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4676 - val_loss: 0.4318\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4656 - val_loss: 0.4309\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4644 - val_loss: 0.4320\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4641 - val_loss: 0.4311\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4638 - val_loss: 0.4325\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4636 - val_loss: 0.4296\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4632 - val_loss: 0.4296\n",
      "Train on 24000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.5733 - val_loss: 0.4778\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4990 - val_loss: 0.4613\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4872 - val_loss: 0.4540\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4820 - val_loss: 0.4521\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4794 - val_loss: 0.4494\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4780 - val_loss: 0.4488\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4774 - val_loss: 0.4488\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4770 - val_loss: 0.4483\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4768 - val_loss: 0.4486\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4766 - val_loss: 0.4486\n",
      "Train on 24000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.7029 - val_loss: 0.5051\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4958 - val_loss: 0.4496\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4729 - val_loss: 0.4402\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4687 - val_loss: 0.4385\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4678 - val_loss: 0.4386\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4676 - val_loss: 0.4376\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4674 - val_loss: 0.4372\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4672 - val_loss: 0.4371\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4673 - val_loss: 0.4374\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4672 - val_loss: 0.4371\n",
      "Train on 24000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.6109 - val_loss: 0.4557\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4793 - val_loss: 0.4370\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4694 - val_loss: 0.4330\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4663 - val_loss: 0.4315\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4650 - val_loss: 0.4302\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4641 - val_loss: 0.4304\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4638 - val_loss: 0.4303\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4635 - val_loss: 0.4297\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4633 - val_loss: 0.4297\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4633 - val_loss: 0.4298\n",
      "Train on 24000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.6211 - val_loss: 0.4707\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4886 - val_loss: 0.4430\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4732 - val_loss: 0.4351\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4680 - val_loss: 0.4323\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4653 - val_loss: 0.4310\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4639 - val_loss: 0.4318\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4632 - val_loss: 0.4310\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 2s - loss: 0.4631 - val_loss: 0.4304\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4630 - val_loss: 0.4303\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 1s - loss: 0.4629 - val_loss: 0.4301\n"
     ]
    }
   ],
   "source": [
    "smallestLabelPath = os.path.join('../data/smallest.csv')\n",
    "smallerLabelPath = os.path.join('../data/smaller.csv')\n",
    "smallLabelPath = os.path.join('../data/small.csv')\n",
    "allDiscretizedPath = os.path.join('../data/all_features_discretized.csv')\n",
    "discreteOnlyPath = os.path.join('../data/discrete_features.csv')\n",
    "\n",
    "discSmallestPath = os.path.join('../data/discSmallest.csv')\n",
    "discSmallerPath = os.path.join('../data/discSmaller.csv')\n",
    "discSmallPath = os.path.join('../data/discSmall.csv')\n",
    "\n",
    "dataTypes = [\"smallest\", \"smaller\", \"small\",\"all disc\", \"disc only\", \"disc smallest\", \"disc smaller\", \"disc small\"]\n",
    "paths = [smallestLabelPath, smallerLabelPath, smallLabelPath, allDiscretizedPath, discreteOnlyPath, discSmallestPath, discSmallerPath, discSmallPath]\n",
    "\n",
    "accuracies = []\n",
    "for i, path in enumerate(paths):\n",
    "    data = pd.read_csv(path)\n",
    "    array = np.array(data)\n",
    "    \n",
    "    X = array[:,:array.shape[1]-1]\n",
    "    Y = array[:,array.shape[1]-1]\n",
    "    \n",
    "    accuracies.append(runModel(X,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 0.27400000000000002, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
